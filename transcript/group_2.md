# Automated transcript of Group Leader Summary

## What is this?
This is an automated transcript of the summary presented by a Group Leader to the other attendees of the Faculty Life Sciences away day on 25 June 2025.

## Group 2
* **Group Leader**: Chris Monit
* **Group Composition**: Federico Celli, Chris Monit, Karland King, James Owler

## Questions (Topic A): Whom are we speaking to when we chat with a large language model?
* Are we actually speaking to a machine?
* Are we speaking to a single conversational counterpart, or to many, or to nobody at all?
* Are we speaking to ourselves?
* What effect does this have on human knowledge and public debate?

## Full Trascript
So, I'm Chris. I'm Rosette representing Group 2, which comprised Fedrico, James, and Carline.

And we had the same questions as the last presentation on to whom are we speaking when we speak to a large language model. And we began by thinking, to what extent is the question itself loaded in favour of one answerer already, by use of the word whom, which implies that the LM, it already has personhood. And so we began exploring the question of whether it's fair to say that LM has personhood and what could confer personhood to a machine, which is mainly trained on text.

And so we asked ourselves, could text itself confer that quality of personhood? And we would say no? And then LLM is just a text completion system, ultimately, and that does not constitute the same thing as we think of philosophically as a person.

But then arguably, the way that the models are trained is somewhat similar to how our brains work, we get text from people and cultural artefacts that we interact with, and that trains our brains, so in that sense, maybe we are text completion systems as well. But we concluded that even a very simple organism will learn from its environment, and we wouldn't call that a a person in itself either, if it's a learning system is not sufficient to be deserving of the pronoun whom in this situation. But then in principle, maybe that could something like this could change in future, because they could have a one-to- one mapping between a human brain and a machine, a perfect representation of a human brain, maybe we will be able to call that a person if we had more complicated models in future.

So on the following question of, are we speaking to a machine? We've decided that we we are speaking to a machine. But in doing so, we are calling on the collective knowledge of many, many people.

And importantly as well, because the models are, they're not pure representations of the things that they've been trained on, they are biased by curation decisions in collecting the training data, and also by the alignment process that companies use when they produce these models. So in that sense, arguably, we are, rather than talking to a machine, we are talking to a corporate structure, which has created the machine, and taking intoation would be very important. And as leads on from that, we think we are speaking to a single conversational counterpart, but that is not a person, that need not be the same thing.

And on the question of whether we are speaking to ourselves, this is a particularly interesting question, because it's arguably no, unless we've written something which has been using the training data. But then it's if we've come from the same cultural environment, that's produced the model and that the model has been trained on, and that sense, we are speaking to ourselves because we are all part of the same soup of language and understanding in the textural world that we live in, and we contribute to. And then finally, on the point of knowledge and debate, we asked ourselves, is it possible to have an LM which is truly unbiassed?

We know that LMch demonstrate bias and things that they output. And is it possible if you, if you did have a perfect representation from training data, which was not leaning one way or the other, and realistically, it would still produce bias because it's ultimately just trying to predict word associations. So some kind of b would still come out.

But the impact on humanledge has been very interesting already, because we've seen that knowledge bases that have used to be referred to regularly, things like stack overflow, for coding, are already less often. And in fact, there's a recent study we've talked about in the news, which found that AI uses associated with the kind of cognitive decline, which is a depressing output already from a crime new technology. And so ultimately we found that as well that there'll be thinking about the kind of public knowledge and sort of impact on culture more widely, there'll be a question about how people come to perceive AI things like LLMs, although in the future, they may not be what we think of now as LLM.

And this question of personhood may come back again when we come to a point where people interacting with them. So for us, these things are new, but they will be a point where we have AI natives who have worked with or used AI things their whole lives. And by then they will be more complicated systems than they are now.

And so there may be we may end up feeling that they' as a culture, that there is personood for these devices, particularly reform friendships with AI systems, and that may end up leading to interesting changes in ethical codes for even protecting AIs in future as well. Is it Martine?