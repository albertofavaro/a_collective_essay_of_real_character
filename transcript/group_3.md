# Automated transcript of Group Leader Summary

## What is this?
This is an automated transcript of the summary presented by a Group Leader to the other attendees of the Faculty Life Sciences away day on 25 June 2025.

## Group 3
* **Group Leader**: Jaisal Kapoor
* **Group Composition**: Oscar Bennett, Spyros Georgakopoulos, Jaisal Kapoor, Arnaud Leredde

## Questions (Topic B): Are we developing AI, or is it developing us?
* Do large language models change how humans think?
* What does it mean for an individual to know something, and are large language models changing this?
* How important is the fact that large language models produce incorrect outputs in relation to our ability to learn from them?
* What long term impact on education, and thus on society, may large language models have?

## Full Trascript
The question that we looked at was, are we developing AI or is it developing us? So we first began by talking about the purpose of developing AI. So we initially we built these tools to kind of make our lives easier to free up our time and allow us to focus on the things that bring us value and meaning.

It also broadly kind of prompts the revaluation of human labour and the purpose of activity, human activity. And then while many LLMs kind of emerge from intellectual curiosity, rather than explicit need, they've had kind of wide knock on or significant societal impacts. So when thinking about whether LLam's change how humans think, I think there was recently an article on called Your Brain on Chat GPT, which argues that we might be we might risk losing some of our reasoning, skills, and a cognitive thinking skills.

So there's the whole use it or lose it piece. But then there's also the argument that what you focus on when you're learning can actually be something that you can actually learn more deeply if you focus on specific tasks or specific topics that interest you. So there are two sides to this debate, but there's also the increase the fact that increasing sophistication of AI kind of can amplify its impact on human thoughts.

So this also brings up the fact that they're specific ethical considerations and safeguards that are crucial hero, because as AI gets more advanced, we need to it reflects what we put into it. So our kind of knowledge and our biases tend to seep in through that, what we're building, what we're developing, which is then into an impacts us. So we need to be very careful in terms of what we're putting into these alalamss.

We also then looked at what it means for an individual to know something and whether LLMs are changing this. So LLM automates access to information, which can then also reduce the emphasis on memorisation and learning. So traditionally, as most of us grew up, there was kind of, in terms of learning, there was an emphasis on memorisation.

There's a risk that this piece could be lost if you have a lot of knowledge at your fingertips, then what is the point of memorising something or trying to attain something if you can access it quickly and you know you'll always have that access and you don't need to hold it in your head. So contributing to knowledge, still does require a deep understanding of fundamental information, not just surface information. So you can access knowledge easily, but contributing to a kind of takes more time to understand and synthesise that information and try to make more meaning out of it.

Again, we do see this in reasoning models, so there is a fact that an LLM can do everything for us that we can retrieve information, but it can also reason for us. And then some of some of us were kind of talking about how there's a risk to surface level learning that because we have access to so much information, we can learn a bit about each topic rather than go deep into a specific topic. And then there's also a risk of potential limitation on creativity, because if you can generate content so easily, why would you then be incentivized to kind of produce things that are new or novel when you when you kind of start relying on these models to give you what you need to create new images, create text, etcera.

And that's a big risk. We then looked at how important is the fact that LLam produce incorrect outputs in relation to our ability to learn from them. So there was again a bit of debate here, because on one end, if you are someone who's, you know, have limited access to information, you want to learn something new, you might tend to to get information from the LLMs and consume it quickly without actually thinking about challenging the source or rationalising what it actually says. But then again, there's a fact that that you should be questioning things, you should be questioning your knowledge, when you read an article in the media, you don't you shouldn't kind of take that at face value on the same way. Having incorrect outputs kind of prompts you to them not look at things and take them at face value and say, okay, yes, this is the truth, but might actually instill some of that reasoning, some of that questioning challenging that it's needed for consuming any sort of information. So this could then foster some of that critical thinking.

Another thing we mentioned was the trend where ELM developers tried to prioritise user satisfaction over factual accuracy, which can then undermine actual learning, because if you, for example, if you put in something in your chat GPT and says, yes, I agree with this. And yes, it always agrees with you all the time, that kind of, again, is a bigger risk and then also we know we notice that that people like Elon Musk were also talking about things like rewriting the internet, and this can be specific threat for, like, large scale dissemination or biased or incorrect information, which then again leads back to the fact that we need to kind of have some sort of ethical safeguards and evaluate what's actually going into these alans. And then just a summarise in the end, we kind of came up with the fact that it's a reciprocal relationship, so we do create AI, but then it does come back to shape us and what's important then at the end is just making sure that we are challenging the information we're receiving, we are making sure that there are safeguards are what we're inputting into this information. and that while we try to use it to democratise knowledge, we're also not keeping all of the impacts and the power with specific people who developed these LLens and these systems to en kind of make sure that some of the benefits are spread out more widely across society.

Yeah, that's where we got to.