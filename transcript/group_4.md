# Automated transcript of Group Leader Summary

## What is this?
This is an automated transcript of the summary presented by a Group Leader to the other attendees of the Faculty Life Sciences away day on 25 June 2025.

## Group 4
* **Group Leader**: Lara Akala
* **Group Composition**: Lara Akala, Ben Jones, James Monsen, Alessandro Alcinesio

## Questions (Topic B): Are we developing AI, or is it developing us?
* Do large language models change how humans think?
* What does it mean for an individual to know something, and are large language models changing this?
* How important is the fact that large language models produce incorrect outputs in relation to our ability to learn from them?
* What long term impact on education, and thus on society, may large language models have?

## Full Trascript
I'm Lara, Team 4, composed of myself, Ben, Arno, no, not Arno. Alessandro, and James. So we started off with the question, do large language models change, how humans think?

Someone in our team came up with an anecdote where they had a friend who has an 11 year-old child whose scorpions are actually anti AI because they believe it robs them of their creativity. and they were tasked with a year six levers song or coming up with a year six Leaver song. Someone in our team said that had they been tasked with the same task, they would have gone straight to AI, but this child was determined not to use AI tools. For some of us, AI is about initial idea generation.

There was another anecdote where someone said, back in the day, teachers had a phobia of Wikipedia because we would copy, or they expected students to copy. There were two main reasons, they didn't like it. Wikipedia can be edited by anyone, and it was considered lazy, but these are both risks that can transfer to an ams.

I said that not copying Wikipedia, but looking at Wikipedia initially for research, just made me really good at using synonyms. And now LLMs do the synonym finding for us Others said that LLMs are really good for the individual doing the initial ID generation themselves, and then LL Lambs or any form of AI being used to help change the tone or add to the content. Ben said he doesn't particularly like to use AI.

He intimately likes to understand each word that he puts on a page, which he knows sounds ironic because he develops AI solutions. But the way he described it is, you don't need to know how a phone works, you just need to know how to use it. And so now, even with data scientists, they don't necessarily need to know how the code works because there are tools that do that for them.

It's not the end of the world. AI can help here. We all believe that we don't need to overuse AI, but we particularly find it used for when we're pressed for time and when there is limited Britain's space to think for ourselves.

What does it mean for an individual to know something and our large language models changing this? Controversially, an individual doesn't need to know anything. You just need to know where to find it.

It helps us prioritise what information to store versus whatnot. James, once again, use the anecdote of he's apparently terrible with directions. So before Google Maps, he just didn't know where to go, I guess.

But now, obviously, you have Google Maps. I argued that there are some people who are just very, very good at directions, even with Google Maps. They have that innate ability, photographic memory to know, you know, where they're going after doing a route once.

And so looping back to the anecdote about children no longer being creative, LLMs might not be destroying your child's creativity. Maybe you just don't have a creative child. Third question is how important is the fact that large language models produce incorrect, sorry incorrect output in relation to our ability to learn from them?

Another controversial take is that new generations have an issue with knowing what's right and what's not, and perhaps older generations have issues with a similar issue where they might see an AI generated video and think that it's real. Alessandro has a friend who teaches children computing. In the beginning, his friend observed that children often had a foundational idea of how computers worked.

But now, children don't know much about the components that go into computers. So there is a growing gap between education and the reality of technology. People know the top player of how LLMs work.

They understand the top layer of abstractness, but in the same way that human textbooks have errors in them, so do LLMs. So we would argue that LLMs make more mistakes than humans. There is no data to back that, but that's what we would argue.

But we tend to treat humans as the ground tooth, but humans are also prone to making loads of mistakes. And finally, what long term impact on education and thus on society may LLM have? We would love to see AI develop us, where curriculums become more personalised to our own style of learning, and where if you don't have access to teachers, AI could be a huge tool, but on the flip side, AI and LMs could also create a larger education disparity, because if you don't currently have access to basic education, the chances are you won't have access to L Ls.

And if you do have access to basic education and teachers, the chances are you will also have access to L Ls, which means that the gap grows because the people that have access to in-person teachers now have a secondary resource in the form of L Ls, and those that don't don't have anything. Thank you for listening.